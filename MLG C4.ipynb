{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ba2f55",
   "metadata": {},
   "source": [
    "https://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65a58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt, seaborn as sb\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# !pip install swifter\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7cd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = glob('./Data/txt_sentoken/pos/*')\n",
    "neg_files = glob('./Data/txt_sentoken/neg/*')\n",
    "\n",
    "def read_txt(FilePath):\n",
    "    with open(FilePath,'r') as fp:\n",
    "        txt = fp.read()\n",
    "    return txt\n",
    "\n",
    "POS_TXTS = [read_txt(i) for i in pos_files]\n",
    "NEG_TXTS = [read_txt(i) for i in neg_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc17643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(POS_TXTS) , len(NEG_TXTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53fce93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" jaws \" is a rare film that grabs your atten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  films adapted from comic books have had plenty...       1\n",
       "1  every now and then a movie comes along from a ...       1\n",
       "2  you've got mail works alot better than it dese...       1\n",
       "3   \" jaws \" is a rare film that grabs your atten...       1\n",
       "4  moviemaking is a lot like being the general ma...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datax = pd.DataFrame(list(zip(POS_TXTS,[1]*len(POS_TXTS))) + list(zip(NEG_TXTS,[0]*len(NEG_TXTS))), columns = ['text','target'])\n",
    "Datax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4b06ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1000\n",
       "0    1000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datax.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072e843a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films adapted from comic books have had plenty of success whether they re about superheroes batman superman spawn or geared toward kids casper or the arthouse crowd ghost world but there s never really been a comic book like from hell before for starters it was created by alan moore and eddie campbell who brought the medium to a whole new level in the mid s with a part series called the watchmen to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd the book or graphic novel if you will is over pages long and includes nearly more that consist of nothing but footnotes in other words don t dismiss this film because of its source if you can get past the whole comic book thing you might find another stumbling block in from hell s directors albert and allen hughes getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in well anything but riddle me this who better to direct a film that s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society the ghetto in question is of course whitechapel in london s east end it s a filthy sooty place where the whores called unfortunates are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision when the first stiff turns up copper peter godley robbie coltrane the world is not enough calls in inspector frederick abberline johnny depp blow to crack the case abberline a widower has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium upon arriving in whitechapel he befriends an unfortunate named mary kelly heather graham say it isn t so and proceeds to investigate the horribly gruesome crimes that even the police surgeon can t stomach i don t think anyone needs to be briefed on jack the ripper so i won t go into the particulars here other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay in the comic they don t bother cloaking the identity of the ripper but screenwriters terry hayes vertical limit and rafael yglesias les mis rables do a good job of keeping him hidden from viewers until the very end it s funny to watch the locals blindly point the finger of blame at jews and indians because after all an englishman could never be capable of committing such ghastly acts and from hell s ending had me whistling the stonecutters song from the simpsons for days who holds back the electric car who made steve guttenberg a star don t worry it ll all make sense when you see it now onto from hell s appearance it s certainly dark and bleak enough and it s surprising to see how much more it looks like a tim burton film than planet of the apes did at times it seems like sleepy hollow the print i saw wasn t completely finished both color and music had not been finalized so no comments about marilyn manson but cinematographer peter deming don t say a word ably captures the dreariness of victorian era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks even though the violence in the film pales in comparison to that in the black and white comic oscar winner martin childs shakespeare in love production design turns the original prague surroundings into one creepy place even the acting in from hell is solid with the dreamy depp turning in a typically strong performance and deftly handling a british accent ians holm joe gould s secret and richardson dalmatians log in great supporting roles but the big surprise here is graham i cringed the first time she opened her mouth imagining her attempt at an irish accent but it actually wasn t half bad the film however is all good r for strong violence gore sexuality language and drug content'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basic Cleaning\n",
    "Datax['text'] = Datax['text'].str.lower()\n",
    "\n",
    "Datax['text'] = Datax['text'].str.replace('[^a-z]',' ',regex=True)\n",
    "Datax['text'] = Datax['text'].str.replace(' +',' ',regex=True)\n",
    "Datax['text'] = Datax['text'].str.strip()\n",
    "Datax.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cee6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "lem = WordNetLemmatizer()\n",
    "ps = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce3e610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif x.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif x.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif x.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def text_process(text,stem=True):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = list(filter(lambda x:x not in sw,tokens))\n",
    "    if stem:\n",
    "        tokens = [ps.stem(t) for t in tokens]\n",
    "    else:\n",
    "        tokens = [lem.lemmatize(t,get_pos(pt)) for t,pt in pos_tag(tokens)]\n",
    "    \n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3c51f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_process('I need the books', stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53a5026a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7904b782d642454db1f7feee8c6b7f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Datax['text'] = Datax['text'].swifter.apply(lambda x:text_process(x,stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0a803fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_WORDS = ' '.join(Datax[Datax.target==1].text.values.tolist())\n",
    "POS_WORDS = word_tokenize(POS_WORDS)\n",
    "\n",
    "NEG_WORDS = ' '.join(Datax[Datax.target==0].text.values.tolist())\n",
    "NEG_WORDS = word_tokenize(NEG_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7c4292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330448, 371976)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NEG_WORDS) , len(POS_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3510c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22871, 24580)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(NEG_WORDS)) , len(set(POS_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4af62cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 6168),\n",
       " ('movie', 3163),\n",
       " ('one', 3156),\n",
       " ('make', 2175),\n",
       " ('character', 2064),\n",
       " ('like', 1931),\n",
       " ('see', 1788),\n",
       " ('get', 1734),\n",
       " ('time', 1576),\n",
       " ('go', 1478)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(POS_WORDS).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db07800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 4980),\n",
       " ('movie', 3818),\n",
       " ('one', 2874),\n",
       " ('make', 2085),\n",
       " ('like', 2009),\n",
       " ('get', 1993),\n",
       " ('character', 1815),\n",
       " ('go', 1564),\n",
       " ('time', 1427),\n",
       " ('even', 1402)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(NEG_WORDS).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "601d167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_DIST = pd.DataFrame(nltk.FreqDist(POS_WORDS).items(),columns = ['word','wordcount_pos'])\n",
    "POS_DIST = POS_DIST.set_index('word')\n",
    "\n",
    "NEG_DIST = pd.DataFrame(nltk.FreqDist(NEG_WORDS).items(),columns = ['word','wordcount_neg'])\n",
    "NEG_DIST = NEG_DIST.set_index('word')\n",
    "\n",
    "\n",
    "COM_DIST = POS_DIST.join(NEG_DIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "448a76b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordcount_pos</th>\n",
       "      <th>wordcount_neg</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>godzilla</th>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-2.148434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lame</th>\n",
       "      <td>15</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-1.916923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>42</td>\n",
       "      <td>229.0</td>\n",
       "      <td>-1.696052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridiculous</th>\n",
       "      <td>22</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-1.679642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>21</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-1.665008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dull</th>\n",
       "      <td>24</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-1.540445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stupid</th>\n",
       "      <td>46</td>\n",
       "      <td>213.0</td>\n",
       "      <td>-1.532651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>22</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-1.504077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>28</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-1.412728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harry</th>\n",
       "      <td>43</td>\n",
       "      <td>144.0</td>\n",
       "      <td>-1.208613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batman</th>\n",
       "      <td>47</td>\n",
       "      <td>157.0</td>\n",
       "      <td>-1.206098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mess</th>\n",
       "      <td>41</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-1.169230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>west</th>\n",
       "      <td>37</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-1.151256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>julie</th>\n",
       "      <td>26</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-1.148623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bore</th>\n",
       "      <td>64</td>\n",
       "      <td>200.0</td>\n",
       "      <td>-1.139434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wordcount_pos  wordcount_neg     ratio\n",
       "word                                              \n",
       "godzilla               14          120.0 -2.148434\n",
       "lame                   15          102.0 -1.916923\n",
       "waste                  42          229.0 -1.696052\n",
       "ridiculous             22          118.0 -1.679642\n",
       "awful                  21          111.0 -1.665008\n",
       "dull                   24          112.0 -1.540445\n",
       "stupid                 46          213.0 -1.532651\n",
       "boring                 22           99.0 -1.504077\n",
       "terrible               28          115.0 -1.412728\n",
       "harry                  43          144.0 -1.208613\n",
       "batman                 47          157.0 -1.206098\n",
       "mess                   41          132.0 -1.169230\n",
       "west                   37          117.0 -1.151256\n",
       "julie                  26           82.0 -1.148623\n",
       "bore                   64          200.0 -1.139434"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COM_DIST = COM_DIST.fillna(1)\n",
    "COM_DIST = COM_DIST[(COM_DIST.wordcount_pos+COM_DIST.wordcount_neg)>100]\n",
    "COM_DIST['ratio'] = np.log(COM_DIST.wordcount_pos/COM_DIST.wordcount_neg)\n",
    "\n",
    "COM_DIST.sort_values(by='ratio', ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "305e24b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordcount_pos</th>\n",
       "      <th>wordcount_neg</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truman</th>\n",
       "      <td>152</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.625985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic</th>\n",
       "      <td>112</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.583005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cameron</th>\n",
       "      <td>133</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.489152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spielberg</th>\n",
       "      <td>102</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.446919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jackie</th>\n",
       "      <td>229</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.444738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulp</th>\n",
       "      <td>86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.409825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memorable</th>\n",
       "      <td>118</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.403389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toy</th>\n",
       "      <td>122</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.402824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarantino</th>\n",
       "      <td>97</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.396657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrific</th>\n",
       "      <td>96</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>123</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.378197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>146</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.346020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lucas</th>\n",
       "      <td>83</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.327798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfectly</th>\n",
       "      <td>130</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.284016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animation</th>\n",
       "      <td>112</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.252763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           wordcount_pos  wordcount_neg     ratio\n",
       "word                                             \n",
       "truman               152           11.0  2.625985\n",
       "titanic              112           23.0  1.583005\n",
       "cameron              133           30.0  1.489152\n",
       "spielberg            102           24.0  1.446919\n",
       "jackie               229           54.0  1.444738\n",
       "pulp                  86           21.0  1.409825\n",
       "memorable            118           29.0  1.403389\n",
       "toy                  122           30.0  1.402824\n",
       "tarantino             97           24.0  1.396657\n",
       "terrific              96           24.0  1.386294\n",
       "political            123           31.0  1.378197\n",
       "excellent            146           38.0  1.346020\n",
       "lucas                 83           22.0  1.327798\n",
       "perfectly            130           36.0  1.284016\n",
       "animation            112           32.0  1.252763"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COM_DIST.sort_values(by='ratio', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb387de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac916cc",
   "metadata": {},
   "source": [
    "\n",
    "## Text Preprocessing\n",
    "\n",
    "The text needs to be transformed to vectors so as the algorithms will be able make predictions. In this case it will be used the Term Frequency – Inverse Document Frequency (TFIDF) weight to evaluate __how important a word is to a document in a collection of documents__.\n",
    "\n",
    "After removing __punctuation__ and __lower casing__ the words, importance of a word is determined in terms of its frequency.\n",
    "\n",
    "\n",
    "\n",
    "### “Term Frequency – Inverse Document Frequency \n",
    "\n",
    "__TF-IDF__ is the product of the __TF__ and __IDF__ scores of the term.<br><br> $$\\text{TF-IDF}=\\frac{\\text{TF}}{\\text{IDF}}$$<br>\n",
    "\n",
    "__Term Frequency :__ This summarizes how often a given word appears within a document.\n",
    "\n",
    "$$\\text{TF} = \\frac{\\text{Number of times the term appears in the doc}}{\\text{Total number of words in the doc}}$$<br><br>\n",
    "__Inverse Document Frequency:__ This downscales words that appear a lot across documents. A term has a high IDF score if it appears in a few documents. Conversely, if the term is very common among documents (i.e., “the”, “a”, “is”), the term would have a low IDF score.<br>\n",
    "\n",
    "$$\\text{IDF} = \\ln\\left(\\frac{\\text{Number of docs}}{\\text{Number docs the term appears in}} \\right)$$<br>\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The higher the TFIDF score, the rarer the term is. For instance, in a Mortgage complaint the word _mortgage_ would be mentioned fairly often. However, if we look at other complaints, _mortgage_ probably would not show up in many of them. We can infer that _mortgage_ is most probably an important word in Mortgage complaints as compared to the other products. Therefore, _mortgage_ would have a high TF-IDF score for Mortgage complaints.\n",
    "\n",
    "TfidfVectorizer class can be initialized with the following parameters:\n",
    "* __min_df__: remove the words from the vocabulary which have occurred in less than ‘min_df’ number of files.\n",
    "* __max_df__: remove the words from the vocabulary which have occurred in more than _‘max_df’ * total number of files in corpus_.\n",
    "* __sublinear_tf__: set to True to scale the term frequency in logarithmic scale.\n",
    "* __stop_words__: remove the predefined stop words in 'english'.\n",
    "* __use_idf__: weight factor must use inverse document frequency.\n",
    "* __ngram_range__: (1, 2) to indicate that unigrams and bigrams will be considered\n",
    "* __max_features__: maximum number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfcee7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Datax.text.values\n",
    "y = Datax.target.values\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e55aebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TfidfVectorizer(max_features = 500)\n",
    "x_train = td.fit_transform(x_train)\n",
    "x_test = td.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3af38906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 500), (400, 500))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape , x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba4bf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "## Train\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "## Test\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c5d255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC : 0.7675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       200\n",
      "           1       0.78      0.75      0.76       200\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "clf_report = classification_report(y_test,y_pred)\n",
    "print(F'ACC : {accuracy_score(y_test,y_pred)}')\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686462ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564464eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
